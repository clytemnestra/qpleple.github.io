<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>On Smoothing In Topic Coherence Measures</title>
    <meta name="viewport" content="width=device-width">

    <link rel="stylesheet" href="/css/bootstrap.min.css">
    <link rel="stylesheet" href="/css/font-awesome.min.css">
    <link rel="stylesheet" href="/css/syntax.css">
    <link rel="stylesheet" href="/css/style.css">

    
        
    
        
            <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
              tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']], processEscapes:true}
            });
            </script>
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
            </script>
        
    

    <script type="text/javascript">
        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-18013119-1']);
        _gaq.push(['_trackPageview']);

        (function() {
          var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
          ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
          var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();
    </script>
</head>
<body>
 
<div class="container">
  <div id="page" style="position: relative">
    
    
    <h1>On Smoothing In Topic Coherence Measures</h1>
    
    <div id="post-content">
        <p>[<a href="/bib#Stevens12">Stevens12</a>] performed an extensive study of the these two measures on one dataset, New York Times articles (92,600 New York Times articles from 2003 and a vocabulary size of 35,836 tokens after removing the ones occurring less than 200 times throughout the corpus), in order to compare the three models: LDA, LSA with SVD, and LSA with NMF.</p>

<p>However, they did not use the formulations of the measures used by their original authors.</p>

<h3>UMass measure</h3>

<p>For the UMass measure, they introduced a free parameter $\epsilon$, instead of just one, for smoothing in the pairwise scoring function
    $$
    \text{score}_{\text{UMass}}(w_i, w_j) =
    \log
    \frac{D(w_i, w_j) + \epsilon}{D(w_i)}
    $$
and tried both $\epsilon = 1$ and $\epsilon = 10^{-12}$. Setting $\epsilon = 10^{-12}$ seems to over-penalize pairs that never occur together, i.e. when $D(w_i, w_j) = 0$, as it will decrease the score of that pair by 12
    $$
    \text{score}_{\text{UMass}}(w_i, w_j)
    = \log \frac{\epsilon}{D(w_i)}
    = -12 - \log D(w_i)
    $$
which is very large in the log space of document counts. It is also equivalent to say that we would have to see $10^{12}$ more documents to see the two words appearing only once together. Having $\epsilon = 1$ looks more reasonable as it is treating pairs that appear once throughout the corpus, i.e. when $D(w_i, w_j) = 1$, and pairs that never appear, i.e. when $D(w_i, w_j) = 0$, roughly in the same order of magnitude.</p>

<h3>UCI measure</h3>

<p>For the UCI measure, they introduced a smoothing $\epsilon$ in the pairwise score that the original authors did not
    $$ \text{score}_{\text{UCI}}(w_i, w_j)  = \log \frac{p(w_i, w_j) + \epsilon}{p(w_i)p(w_j)}$$
with $\epsilon$ initially set to one. Here, we are at a different scale as we are dealing with probabilities and not counts. As $\epsilon = 1$ is likely to be huge compared to $p(w_i, w_j)$, the smoothing parameter artificially increases the topic coherence, and not even by the same amount for all pairs of words. Then, using $\epsilon = 10^{-12}$, which is likely to be smaller than $p(w_i, w_j)$, caused big changes, and therefore the authors concluded that coherence measures depend heavily on smoothing $\epsilon$.</p>

<p>Nonetheless a smoothing parameter is required to avoid taking the logarithm of zero. A reasonable choice for smoothing is to assume that every pair of words is present at least once in the corpus, and compute the empirical probability
    $$ p(w_i, w_j) = \frac{D_{\text{Wikipedia}}(w_i, w_j) + 1}{D_{\text{Wikipedia}}}$$
which is the same smoothing as the UMass measure.</p>

    </div>

    <div id="card-post">
        <img class="img-circle pull-left" style="height: 50px" src="/img/picture.jpg">
        <a href="/">Quentin Plepl√©</a> <br>
        <span class="muted">
            May 2013
        </span>
    </div>

</div>

<div id="disqus_thread" style="padding: 60px 40px 20px 40px"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'qpleple'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>

</body>
</html>
