<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Bayesian Inference</title>
    <meta name="viewport" content="width=device-width">

    <link rel="stylesheet" href="/css/bootstrap.min.css">
    <link rel="stylesheet" href="/css/font-awesome.min.css">
    <link rel="stylesheet" href="/css/syntax.css">
    <link rel="stylesheet" href="/css/style.css">

    
        
    
        
            <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
              tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']], processEscapes:true}
            });
            </script>
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
            </script>
        
    

    <script type="text/javascript">
        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-18013119-1']);
        _gaq.push(['_trackPageview']);

        (function() {
          var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
          ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
          var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();
    </script>
</head>
<body>
 
<div class="container">
  <div id="page" style="position: relative">
    
    
    <h1>Bayesian Inference</h1>
    
    <div id="post-content">
        <p>Suppose we have a probabilistic model with observations $\boldsymbol x$ and hidden variables $\boldsymbol z$. We want to infer the hidden variables $\boldsymbol z$ that can best explain the observations $\boldsymbol x$. In the Bayesian framework, we treat parameters as hidden random variables as well.</p>

<h3>Maximum Likelihood</h3>

<p>The common way to perform inference is to compute the Maximum Likelihood estimate
$$
    \hat{\boldsymbol z}_{\text{ML}}
    = \mathop{\arg\!\max}_{\boldsymbol z} p(\boldsymbol x | \boldsymbol z)
$$
which is the set of hidden variables that maximizes the likelihood $p(\boldsymbol x | \boldsymbol z)$ of the observed data.</p>

<h3>Maximum a posteriori</h3>

<p>A Bayesian approach will assume some prior knowledge about the hidden variables: a distribution $p(\boldsymbol z)$ over the space of hidden variables $\boldsymbol z$. The likelihood $p(\boldsymbol x | \boldsymbol z)$ gets weighted by the prior knowledge $p(\boldsymbol z)$ to give the posterior $p(\boldsymbol z | \boldsymbol x)$:
    $$
        \text{posterior} \ \propto \ \text{likelihood} \ \times \ \text{prior}
        \qquad
        \text{or here}
        \qquad
        p(\boldsymbol z | \boldsymbol x)
        \ \propto \
        p(\boldsymbol x | \boldsymbol z)
        \ \times \
        p(\boldsymbol z).
    $$
The maximum a posteriori (MAP) estimate
    $$
        \hat{\boldsymbol z}_{\text{MAP}}
        = \mathop{\arg\!\max}_z p(\boldsymbol x | \boldsymbol z) p(\boldsymbol z)
        = \mathop{\arg\!\max}_z p(\boldsymbol z | \boldsymbol x)
    $$
is the set of hidden variables that maximizes the posterior. The main advantage of this approach is that in order to maximize $p(\boldsymbol z | \boldsymbol x)$, we don&#39;t need to compute the normalizing constant 
    $$
        p(\boldsymbol x)
        = \int_{\boldsymbol z}
        p(\boldsymbol x | \boldsymbol z) p(\boldsymbol z)
        \text{d}\boldsymbol z
    $$
whereas it is needed for the full Bayesian approach, as we will see in the next paragraph. This normalizer can become intractable when models get complicated. The MAP method is therefore sometimes described as poor man&#39;s Bayesian inference [<a href="/bib#Tzikas08">Tzikas08</a>] as this is a way of including prior knowledge without having to pay the expensive price of computing $p(\boldsymbol x)$. </p>

<h3>Full Bayesian approach</h3>

<p>In the full Bayesian approach, we don&#39;t only want one estimate $\hat{\boldsymbol z}$ of the hidden variables, but the entire distribution over them, the posterior distribution $p(\boldsymbol z | \boldsymbol x)$.</p>

<p>Every time we pick one specific value for a parameter in a model, we are making an approximation. As we compose models, approximations get amplified at every layer of the model. Feeding the entire parameters distribution in the next layer, instead of a point estimate, will increase the added value of this layer to the model.</p>

<p>In this approach, the normalizer $p(\boldsymbol x)$ becomes intractable as the model gets more complex, making it impossible to compute the posterior distribution. Often, the best we can do is approximate the posterior.</p>

    </div>

    <div id="card-post">
        <img class="img-circle pull-left" style="height: 50px" src="/img/picture-2.jpg">
        <a href="/">Quentin Plepl√©</a> <br>
        <span class="muted">
            April 2013
        </span>
    </div>

</div>

<div id="disqus_thread" style="padding: 60px 40px 20px 40px"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'qpleple'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>

</body>
</html>
