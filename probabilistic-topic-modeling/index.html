<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Probabilistic Topic Modeling</title>
    <meta name="viewport" content="width=device-width">

    <link rel="stylesheet" href="/css/bootstrap.min.css">
    <link rel="stylesheet" href="/css/font-awesome.min.css">
    <link rel="stylesheet" href="/css/syntax.css">
    <link rel="stylesheet" href="/css/style.css">

    
        
    
        
            <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
              tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']], processEscapes:true}
            });
            </script>
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
            </script>
        
    

    <script type="text/javascript">
        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-18013119-1']);
        _gaq.push(['_trackPageview']);

        (function() {
          var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
          ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
          var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();
    </script>
</head>
<body>
 
<div class="container">
  <div id="page" style="position: relative">
    
    
    <h1>Probabilistic Topic Modeling</h1>
    
    <div id="post-content">
        <h3>Probabilistic models</h3>

<p>LSA represents topics as points in Euclidean space and documents as linear combination of topics. Probabilistic topic models differ from LSA by representing topics as distributions over words, and documents as probabilistic mixtures of topics. Formally, given a vocabulary of $W$ words, each topic $k = 1, ..., K$ has a word-distribution $\boldsymbol \phi_k \in \Delta_W$, with $\Delta_n$ the simplex of dimension $n$, and each document $d = 1, ..., D$ has a topic-distribution $\boldsymbol \theta_d \in \Delta_K$. Call $\boldsymbol \Phi$ the matrix with rows $\boldsymbol \phi_k$ and $\boldsymbol \Theta$ the matrix with rows $\boldsymbol \theta_d$.</p>

<p>By estimating the model parameters $(\boldsymbol \Phi, \boldsymbol \Theta)$ of the model, we discover new knowledge about the corpus; topics are represented by the word-distributions $\boldsymbol \phi_k$, and each document is tagged with discovered topics, which is represented by $\boldsymbol \theta_d$.</p>

<h3>Probabilistic Latent Semantic Indexing</h3>

<p>Probabilistic Latent Semantic Indexing (pLSI) is described as a generative process [<a href="/bib#Hofmann99">Hofmann99</a>], a procedure that probabilistically generates documents given the parameters of the model. Parameters of the model are then learned by Bayesian inference.</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">procedure GenerativePLSI(document-tagging theta_d, topics phi_k):
  for document d = 1..D:
    for position i = 1..N_d in document d:
      draw a topic z_di from Discrete(theta_d)
      draw a word w_di from Discrete(phi_z_di)

  return counts n_dw = sum_i I(w_di = w)
</code></pre></div>
<p>Although the formulation of the model is probabilistic, [<a href="/bib#Ding08">Ding08</a>] proved the equivalence between pLSI and NMF, by showing that they both optimize the same objective function. As they are different algorithms, they will navigate in the parameters space differently. It is possible to design an hybrid algorithm alternating between NMF and pLSI, every time jumping out of the local optimum of the other method. </p>

<h3>Latent Dirichlet Allocation</h3>

<p>[<a href="/bib#Blei03">Blei03</a>] extended pLSI by adding a symmetric Dirichlet prior $\text{Dir}(\alpha)$ on topic-distributions $\boldsymbol \theta_d$ of documents and derived a variational EM algorithm for the Bayesian inference.
[<a href="/bib#Griffiths02a">Griffiths02a</a>] [<a href="/bib#Griffiths02b">Griffiths02b</a>] went further by adding a symmetric Dirichlet prior $\text{Dir}(\beta)$ on topics $\boldsymbol \phi_k$, and derived a Gibbs sampler for the Bayesian inference. Here is the generative process of LDA.</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">procedure GenerativeLDA(document-tagging smoothing alpha, topic smoothing beta):
  for topic k = 1..K:
    draw a word-distribution phi_k from Dir(beta)
  for document d = 1..D:
    draw a topic-distribution theta_d from Dir(alpha)
    for position i = 1..N_d in document d:
      draw a topic z_di from Discrete(theta_d)
      draw a word w_di from Discrete(phi_z_di)

  return counts n_dw = sum_i I(w_di = w)
</code></pre></div>
<p>Bayesian model can also be described graphically in plate notation which helps to understand the dependencies between random variables.</p>

<p>We suppose documents are generated according to this generative model and we want to estimate values for a set a parameters $(\boldsymbol \Phi, \boldsymbol \Theta)$ that can best explain the set of observations: the word counts $n_{di}$.</p>

<p>Theoretically, we could learn hyperparameters $\alpha$ and $\beta$ using Newton-Raphson method [<a href="/bib#Blei03">Blei03</a>]. But usually, hyperparameters are fixed heuristically to simplify the algorithm and make it converge faster. Common values are $\alpha = \frac 1 K$ and $\beta = 0.1$ [<a href="/bib#Steyvers06">Steyvers06</a>].</p>

    </div>

    <div id="card-post">
        <img class="img-circle pull-left" style="height: 50px" src="/img/picture-2.jpg">
        <a href="/">Quentin Plepl√©</a> <br>
        <span class="muted">
            April 2013
        </span>
    </div>

</div>

<div id="disqus_thread" style="padding: 60px 40px 20px 40px"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'qpleple'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>

</body>
</html>
